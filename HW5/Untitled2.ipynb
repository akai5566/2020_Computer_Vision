{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: (1500, 224, 224)\n",
      "Testing data size: (150, 224, 224)\n",
      "Train label size: (1500,)\n",
      "[ 0.  0.  0. ... 14. 14. 14.]\n",
      "Test label size: (150,)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  3.  3.  3.  3.  3.  3.\n",
      "  3.  3.  3.  3.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  7.  7.\n",
      "  7.  7.  7.  7.  7.  7.  7.  7.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "  9.  9.  9.  9.  9.  9.  9.  9.  9.  9. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 12. 12. 12. 12. 12. 12.\n",
      " 12. 12. 12. 12. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 14. 14. 14. 14.\n",
      " 14. 14. 14. 14. 14. 14.]\n",
      "Training data size: (1500, 224, 224, 1)\n",
      "Testing data size: (150, 224, 224, 1)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/300\n",
      "47/47 [==============================] - 87s 2s/step - loss: 2.7109 - accuracy: 0.0653 - val_loss: 2.7020 - val_accuracy: 0.0800\n",
      "Epoch 2/300\n",
      "47/47 [==============================] - 84s 2s/step - loss: 2.5641 - accuracy: 0.1533 - val_loss: 2.4443 - val_accuracy: 0.2067\n",
      "Epoch 3/300\n",
      "47/47 [==============================] - 83s 2s/step - loss: 2.3547 - accuracy: 0.2293 - val_loss: 2.2387 - val_accuracy: 0.2467\n",
      "Epoch 4/300\n",
      "47/47 [==============================] - 84s 2s/step - loss: 2.2881 - accuracy: 0.2553 - val_loss: 2.1946 - val_accuracy: 0.2667\n",
      "Epoch 5/300\n",
      "47/47 [==============================] - 88s 2s/step - loss: 2.2031 - accuracy: 0.2767 - val_loss: 2.0674 - val_accuracy: 0.2933\n",
      "Epoch 6/300\n",
      "47/47 [==============================] - 88s 2s/step - loss: 2.1628 - accuracy: 0.2880 - val_loss: 2.0932 - val_accuracy: 0.2600\n",
      "Epoch 7/300\n",
      "47/47 [==============================] - 82s 2s/step - loss: 2.0974 - accuracy: 0.3193 - val_loss: 2.0466 - val_accuracy: 0.3600\n",
      "Epoch 8/300\n",
      "47/47 [==============================] - 84s 2s/step - loss: 2.0786 - accuracy: 0.3280 - val_loss: 1.9852 - val_accuracy: 0.3133\n",
      "Epoch 9/300\n",
      "47/47 [==============================] - 86s 2s/step - loss: 2.0173 - accuracy: 0.3420 - val_loss: 2.0133 - val_accuracy: 0.3467\n",
      "Epoch 10/300\n",
      "47/47 [==============================] - 98s 2s/step - loss: 2.0113 - accuracy: 0.3467 - val_loss: 2.0104 - val_accuracy: 0.2933\n",
      "Epoch 11/300\n",
      "47/47 [==============================] - 143s 3s/step - loss: 1.9538 - accuracy: 0.3833 - val_loss: 1.9282 - val_accuracy: 0.3333\n",
      "Epoch 12/300\n",
      "47/47 [==============================] - 141s 3s/step - loss: 1.8978 - accuracy: 0.3960 - val_loss: 1.8070 - val_accuracy: 0.4133\n",
      "Epoch 13/300\n",
      "47/47 [==============================] - 91s 2s/step - loss: 1.8539 - accuracy: 0.3853 - val_loss: 1.8049 - val_accuracy: 0.3667\n",
      "Epoch 14/300\n",
      "47/47 [==============================] - 90s 2s/step - loss: 1.7556 - accuracy: 0.4340 - val_loss: 1.8623 - val_accuracy: 0.3667\n",
      "Epoch 15/300\n",
      "47/47 [==============================] - 86s 2s/step - loss: 1.7141 - accuracy: 0.4520 - val_loss: 1.9576 - val_accuracy: 0.3400\n",
      "Epoch 16/300\n",
      "47/47 [==============================] - 140s 3s/step - loss: 1.6599 - accuracy: 0.4527 - val_loss: 1.6235 - val_accuracy: 0.4733\n",
      "Epoch 17/300\n",
      "47/47 [==============================] - 89s 2s/step - loss: 1.6245 - accuracy: 0.4693 - val_loss: 1.5230 - val_accuracy: 0.4867\n",
      "Epoch 18/300\n",
      "47/47 [==============================] - 114s 2s/step - loss: 1.5702 - accuracy: 0.4853 - val_loss: 1.6293 - val_accuracy: 0.4333\n",
      "Epoch 19/300\n",
      "47/47 [==============================] - 204s 4s/step - loss: 1.5396 - accuracy: 0.4973 - val_loss: 1.4519 - val_accuracy: 0.5133\n",
      "Epoch 20/300\n",
      "47/47 [==============================] - 186s 4s/step - loss: 1.5273 - accuracy: 0.4927 - val_loss: 1.6155 - val_accuracy: 0.5000\n",
      "Epoch 21/300\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "from __future__ import print_function\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "\n",
    "\n",
    "import vis\n",
    "import math\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from PIL import Image\n",
    "import keras\n",
    "from keras.applications import VGG16\n",
    "from keras import optimizers\n",
    "from keras import activations\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from vis.visualization import visualize_activation\n",
    "from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D, GlobalMaxPooling2D, BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#=====Load training data=========\n",
    "\n",
    "train_data = []\n",
    "path = \"hw5_data/train/**/*\"\n",
    "\n",
    "#====Get the file name which under the folder===\n",
    "files = glob.glob(path)\n",
    "\n",
    "for File in files:\n",
    "    im = Image.open(File)\n",
    "    \n",
    "######Resize the images to a common size######\n",
    "    im = im.resize((224,224),Image.BICUBIC) # 224 initial\n",
    "    im = np.asarray(im).astype('float32')\n",
    "    \n",
    "#=======Normalize the value of images========\n",
    "    im = im/255.\n",
    "    \n",
    "#=== Transpose the images to channel first, it will be fast at training ===\n",
    "    train_data.append(im)#np.transpose(im, (2, 0, 1)))\n",
    "    \n",
    "train_data = np.asarray(train_data)\n",
    "print(\"Training data size:\",train_data.shape)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#=====Load testing data=========\n",
    "\n",
    "test_data = []\n",
    "path = \"hw5_data/test/**/*\"\n",
    "\n",
    "#====Get the file name which under the folder===\n",
    "files = glob.glob(path)\n",
    "\n",
    "for File in files:\n",
    "    im = Image.open(File)\n",
    "    \n",
    "######Resize the images to a common size######\n",
    "    im = im.resize((224,224),Image.BICUBIC)\n",
    "    im = np.asarray(im).astype('float32')\n",
    "    \n",
    "#=======Normalize the value of images========\n",
    "    im = im/255.\n",
    "    \n",
    "#=== Transpose the images to channel first, it will be fast at training ===\n",
    "    test_data.append(im)#np.transpose(im, (2, 0, 1)))\n",
    "    \n",
    "test_data = np.asarray(test_data)\n",
    "print(\"Testing data size:\",test_data.shape)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "train_label = np.zeros((1500))\n",
    "for i in range(15):\n",
    "    train_label[i*100:100*i+100] = i\n",
    "print(\"Train label size:\",train_label.shape)\n",
    "print(train_label)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "test_label = np.zeros((150))\n",
    "for i in range(15):\n",
    "    test_label[i*10:10*i+10] = i\n",
    "print(\"Test label size:\",test_label.shape)\n",
    "print(test_label)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 15\n",
    "epochs = 100\n",
    "\n",
    "img_rows, img_cols = 224, 224\n",
    "img_shape = (img_rows, img_cols)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "train_data = train_data.reshape(train_data.shape[0], img_rows, img_cols, 1)\n",
    "test_data = test_data.reshape(test_data.shape[0], img_rows, img_cols, 1)\n",
    "print(\"Training data size:\",train_data.shape)\n",
    "print(\"Testing data size:\",test_data.shape)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "train_label = keras.utils.to_categorical(train_label, num_classes)\n",
    "test_label = keras.utils.to_categorical(test_label, num_classes)\n",
    "print(train_label)\n",
    "print(test_label)\n",
    "\n",
    "\n",
    "# ### Build our Model\n",
    "\n",
    "# In[63]:\n",
    "\n",
    "\n",
    "'''Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n",
    "(it's still underfitting at that point, though).\n",
    "'''\n",
    "\n",
    "#from _future_ import print_function\n",
    "#import keras\n",
    "#from keras.datasets import cifar10\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "#from keras.layers import Conv2D, MaxPooling2D\n",
    "#import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 15\n",
    "epochs = 300\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "#save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "#model_name = 'keras_cifar10_trained_model.h5'\n",
    "#\n",
    "## The data, split between train and test sets:\n",
    "#(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "#print('x_train shape:', x_train.shape)\n",
    "#print(x_train.shape[0], 'train samples')\n",
    "#print(x_test.shape[0], 'test samples')\n",
    "#\n",
    "## Convert class vectors to binary class matrices.\n",
    "#y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "#\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=train_data.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#x_train = x_train.astype('float32')\n",
    "#x_test = x_test.astype('float32')\n",
    "#x_train /= 255\n",
    "#x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(train_data)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(train_data, train_label,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(test_data, test_label),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "#if not os.path.isdir(save_dir):\n",
    "#    os.makedirs(save_dir)\n",
    "#model_path = os.path.join(save_dir, model_name)\n",
    "#model.save(model_path)\n",
    "#print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(test_data, test_label, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "\n",
    "\n",
    "# In[64]:\n",
    "\n",
    "\n",
    "K.set_value(model.optimizer.lr, 0.00001)\n",
    "model.fit_generator(datagen.flow(train_data, train_label,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=50,\n",
    "                        validation_data=(test_data, test_label),\n",
    "                        workers=4)\n",
    "\n",
    "\n",
    "# In[65]:\n",
    "\n",
    "\n",
    "K.set_value(model.optimizer.lr, 0.000001)\n",
    "model.fit_generator(datagen.flow(train_data, train_label,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=50,\n",
    "                        validation_data=(test_data, test_label),\n",
    "                        workers=4)\n",
    "\n",
    "\n",
    "# In[66]:\n",
    "\n",
    "\n",
    "# Testing\n",
    "score = model.evaluate(test_data, test_label, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
